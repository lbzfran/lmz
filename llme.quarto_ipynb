{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"LM based on Me\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Setup\n"
      ],
      "id": "8c6b74cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import regex as re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n"
      ],
      "id": "831b1188",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(36432)"
      ],
      "id": "f77872b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Importing"
      ],
      "id": "46d4fe95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def read_document(filepath: str) -> pd.DataFrame:\n",
        "\n",
        "    # put any filters here\n",
        "\n",
        "    lines = []\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    filtered_lines = []\n",
        "    for line in lines:\n",
        "        if (\n",
        "            \"ENTRY\" not in line and\n",
        "            \"CHAPTER\" not in line\n",
        "        ):\n",
        "            filtered_lines.append(line)\n",
        "\n",
        "    df = pd.DataFrame(filtered_lines)\n",
        "\n",
        "    return df"
      ],
      "id": "c7cfd29e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_stories = {}\n",
        "data_directory = Path(\"./data\")\n",
        "\n",
        "for file in data_directory.glob(\"*.txt\"):\n",
        "    filename = file.stem\n",
        "    all_stories[filename] = read_document(file)\n",
        "\n",
        "all_stories[filename].head()"
      ],
      "id": "681cc1d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this simple code snippet, we are able to import as many text files\n",
        "as we want from our data directory, filter out lines as necessary, and\n",
        "convert them into a DataFrame.\n",
        "\n",
        "## Tokenizing the text\n",
        "\n",
        "This implementation will focus on using Byte Pair Encoding tokenization,\n",
        "which encodes a fixed size of tokens.\n",
        "It is a healthy balance between simply tokenizing per character, or\n",
        "tokenizing per word, which in either case may yield too little or too\n",
        "many tokens.\n"
      ],
      "id": "c09d7ac6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_sequence = \"\"\n",
        "for story in all_stories.keys():\n",
        "    text_sequence += \" \".join(all_stories[story][0].values)\n",
        "\n",
        "print(f\"size of text_sequence: {len(text_sequence)}\")"
      ],
      "id": "635d1d4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sys.path.append('./lib')\n",
        "from minbpe import BasicTokenizer\n",
        "\n",
        "tokenizer = BasicTokenizer()\n",
        "tokenizer.train(text_sequence, vocab_size=1024)"
      ],
      "id": "fae6ebcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking a look at the token sequences now:"
      ],
      "id": "1ed3f6e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocab = tokenizer.vocab\n",
        "\n",
        "vocab"
      ],
      "id": "2ea479e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer.encode(\"What the\")"
      ],
      "id": "7a23ed21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer.decode([120, 543, 222, 76])"
      ],
      "id": "9f9a6b27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking pretty spicy.\n",
        "\n",
        "Let's also append some special tokens to the vocab.\n",
        "\n",
        "This will be useful for training later on.\n"
      ],
      "id": "c66c915a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_vocab_id = list(vocab.keys())[-1]\n",
        "tokenizer.special_tokens = {\n",
        "    max_vocab_id + 1: \"<<startoftext>>\",\n",
        "    max_vocab_id + 2: \"<<separator>>\",\n",
        "    max_vocab_id + 3: \"<<endoftext>>\",\n",
        "    max_vocab_id + 4: \"<<unk>>\"\n",
        "}"
      ],
      "id": "18817e80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(tokenizer.encode(text_sequence))"
      ],
      "id": "adaf17cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer.save(file_prefix=\"./output/tokenizer/da_tokenizer\")"
      ],
      "id": "d3dc05ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Model\n",
        "\n",
        "We will need both an encoder and a decoder.\n",
        "\n",
        "A decoder will consist of the following components:\n",
        "- token embedding (represent a token with a vector)\n",
        "- positional encoding (preserving token orders)\n",
        "- self attention (keep track of relation between tokens)\n",
        "- residual connections\n",
        "- layer normalization\n",
        "\n",
        "Parameters of a decoder:\n",
        "- block size\n",
        "- embedding size\n",
        "- number of heads & head size\n",
        "- number of blocks (layers)\n"
      ],
      "id": "0a386ddc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
        "    vocab = tokenizer.vocab\n",
        "    special_tokens = tokenizer.special_tokens\n",
        "\n",
        "    return len(vocab) + len(special_tokens)"
      ],
      "id": "5f95a168",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# hyperparameters\n",
        "block_size = 256\n",
        "embed_size = 384\n",
        "head_count = 6\n",
        "layer_count = 6\n",
        "dropout = 0.2\n",
        "vocab_size = get_vocab_size(tokenizer)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "id": "393c5570",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        input size (batch, time-step, channels)\n",
        "        output size (batch, time-step, head size)\n",
        "        \"\"\"\n",
        "\n",
        "        _, T, _ = x.shape\n",
        "\n",
        "        k = self.key(x) # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (or \"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        weights = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        weights = weights.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = weights @ v\n",
        "        return out"
      ],
      "id": "2ae0f7d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_head: int, head_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(head_size + num_heads, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out"
      ],
      "id": "e33785b7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "D:\\scout\\Code\\llme\\.venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}